\documentclass{jarticle}
\usepackage{amsmath, url}
\title{私のための統計学入門}
\author{阿部 興\footnote{「あべ こう」と読む}}
\西暦
\begin{document}
\maketitle

\newcommand{\KL}{D_{\mathrm KL}}

\section*{記法}

$\log(x)$ は自然対数を表す.

\section{確率の話}
ここでは主に大数の法則, 中心極限定理, カルバック・ライブラ情報量について勉強する.

\subsection{大数の法則}

いろいろな本に書いてあるので省略する.

\subsection{中心極限定理}

中心極限定理の証明には, 特性関数やモーメント母関数を使うことが多い. 

テイラー展開を使ったより簡単な証明が, 黒木 (2017)\cite{Kuroki2017} にある. 

\subsection{カルバック・ライブラ情報量}
統計的推測のために重要な役割を果たす量の1つにカルバック・ライブラ情報量がある.
カルバック・ライブラ情報量$\KL(p\|q)$は離散型の確率関数$p(x)$と$q(x)$に対しては, 
\begin{align}
\KL(p\|q) = -\sum_{x} p(x) \log \frac{q(x)}{p(x)}
\end{align}
連続型の確率密度関数$p(x)$と$q(x)$に対しては, 
\begin{align}
\KL(p\|q) = -\int p(x) \log \frac{q(x)}{p(x)} \, dx
\end{align}
と定義される. 
赤池（1980）\cite{Akaike1980} に習って, カルバック・ライブラ情報量の直感的な意味を考える. 結論を先取りすると, カルバック・ライブラ情報量は「サンプルを生成した分布が $q(x)$ のとき, 経験分布がほぼ $p(x)$ となる確率の対数のサンプルサイズ分の 1 の符号反転」と解釈できる. このカギカッコの中身の意味がわかるようになることがこの節の目標である.

状態 $i=1,\ldots,M$ がそれぞれ $q_i$ の確率で生起する分布と, 状態 $i=1,\ldots,M$ がそれぞれ $p_i$ の確率で生起する分布を考える. この分布からのサンプルを $N$ 個観測して、状態 $i=1,\ldots,M$ が生起した回数をそれぞれ $N_1,\ldots,N_M$ とする. $ N=\sum_{i=1}^{M}N_i$ である. 
$N_1,\ldots,N_M$ のような観測が得られる確率は,
\begin{align}
W=\frac{N!}{N_1! \cdots N_M!}q^{N_1}_1 \cdots q^{N_M}_M
\end{align}
と表せる. （このような分布は多項分布と呼ばれる.）

ここでスターリングの公式
\begin{align}
\log N! \approx N\log N -N
\end{align}
を使って $\log W$ を近似すると
\begin{align}
\log W &\approx (N \log N) - \sum_{i=1}^{M}( N_i \log N_i -N_i) + \sum_i=1^{M} \log q_i \\
&= N \log N - \sum_{i=1}^M N_i (\log N_i - q_i) \\
&= - \sum_{i=1}^M N_i (\log N_i - q_i -\log N ) \\
&= - \sum_{i=1}^M N_i (\log \frac{N_i}{N} - q_i) \\
&= - N \sum_{i=1}^M \frac{N_i}{N} (\log \frac{N_i}{N} - q_i) 
\end{align}
と整理できる.
$p_i = N_i/N$ とおくと
\begin{align}
\log W \approx -N\sum_{i=1}^{M}p_i \log\frac{p_i}{q_i}
\end{align}
という結果を得る.

あらためて考えると $p_i = N_i/N$  は経験的に推定された確率と解釈できる.
$q_i$ は真の確率であったことを思い出すと、$W$ はサンプルを生成した真の分布が $q_i$ のときに $p_i$ のように振る舞う確率と解釈できる. 
$\log W$を $N$ で割り, 符号を反転させると, $-\sum_{i=1}^{M} p_i \log {\frac {q_i}{p_i}}$ となる.
この量は「母集団分布が $q_i$ のとき経験分布がほぼ $p_i$ となる確率の対数のサンプルサイズ分の 1 の符号反転」と解釈できる.

カルバック・ライブラ情報量は次の性質を満たすため, 分布の近さを測る指標となる. 
\begin{align}
\KL(p\|q) \ge 0,
\end{align}
かつ, $\KL(p\|q) = 0$となるのは $p(x)=q(x)$のときに限られる.

\paragraph{注意.}
定義をよく見ればわかるように, カルバック・ライブラ情報量では $\KL(p\|q) =\KL(q\|p)$ は成り立たない. 

%\subsection{サノフの定理}

\section{最尤法}
本節では最尤法と呼ばれる方法の性質について述べる. これについて理解するために, フィッシャー情報量と呼ばれる量が重要になるため, 先にフィッシャー情報量についての性質を述べる. 

\subsection{パラメータが1つの場合}

\subsubsection{スコア関数とフィッシャー情報量}
パラメータ $\theta$ を持つ確率（密度）関数 $p(x|\theta_0)$ について, スコア関数 $S(\theta)$ を次のように定義する. 
\begin{align}
S(\theta) = \frac{d}{d\theta}\log p(x | \theta). 
\end{align}
スコア関数の $p(x|\theta)$ による平均は 0 である.  
\begin{align}
&\int_{-\infty}^{\infty} \frac{d}{d\theta}\log p(x | \theta) p(x|\theta)\, dx \\
&= \int_{-\infty}^{\infty} \frac{\frac{d}{d\theta}p(x | \theta)}{p(x | \theta)} p(x|\theta) \, dx \\
&=\int_{-\infty}^{\infty} \frac{d}{d\theta}p(x | \theta)\, dx \\
&=  \frac{d}{d\theta} \int_{-\infty}^{\infty} p(x | \theta)\, dx\\
&= 0.
\end{align}
従い, スコア関数の分散はスコア関数の2乗の平均に等しい.

フィッシャー情報量を
\begin{align}
I(\theta)=-\int^{\infty}_{-\infty}\left(\frac{d^2}{d \theta^2} \log p(x_i | \theta)\right) p(x|\theta) \, dx
\end{align}
と定義すると, 
\begin{align}
I(\theta)&=-\int^{\infty}_{-\infty}\frac{d}{d \theta} \left(\left(\frac{\frac{d}{d\theta} p(x_i | \theta)}{p(x|\theta)}\right) p(x|\theta) \right) \, dx \\
&=-\int^{\infty}_{-\infty}\left(\left(\frac{\frac{d^2}{d\theta^2} p(x_i | \theta)}{p(x|\theta)} - \frac{ ( \frac{d}{d\theta} p(x_i | \theta))^2}{p(x|\theta)^2 }\right) p(x|\theta) \right) \, dx \\
&=-\int^{\infty}_{-\infty}\left(\frac{\frac{d^2}{d\theta^2} p(x_i | \theta)}{p(x|\theta)}\right) p(x|\theta) \, dx +\int^{\infty}_{-\infty}\left( \frac{ ( \frac{d}{d\theta} p(x_i | \theta))^2}{p(x|\theta)^2 }\right) p(x|\theta) \, dx
\end{align}
スコア関数のときと同様, 第1項は消える. 
第2項は
\begin{align}
\int^{\infty}_{-\infty}\left(\frac{d}{d\theta} \log p(x | \theta)\right)^2 p(x|\theta) \, dx 
\end{align}
と等しい. これはスコア関数の2乗の平均になっている. 
すなわち, スコア関数の分散はフィッシャー情報行列と等しいことがわかった. 

\subsubsection{最尤推定量の性質}
サンプル $x_i$ ($i=1,\ldots n$) が, 独立に同一の確率（密度）関数 $p(x|\theta_0)$ を持つ分布から得られたとする\footnote{ 「独立に同一の確率分布に従う」ことを　i.i.d. (independent and identically distributed) と略すことがある.}. ここで $\theta$ は確率（密度）関数のパラメータである\footnote{このパラメータのことを母数と呼ぶことがあるが, 母数という語は誤解を招くことが多いので, 本稿ではあまりつかわない.}.  
このようなデータに対し, 統計モデル $p(x|\theta)$ を考え, 未知パラメータの $\theta$ を推定したい.

まず, 次の対数尤度比関数 $l_n(\theta)$ を考える. 
\begin{align}
l_n(\theta) = \log \left( \frac{\prod_{i=1}^{n}p(x_i|\theta)}{\prod_{i=1}^{n}p(x_i|\theta_0)} \right) = \sum_{i=1}^{n}\log \left(\frac{\log p(x_i| \theta)}{\log p(x_i| \theta_0)}\right).
\end{align}
これをサンプルサイズ（標本の大きさ） $n$ で割ると大数の法則により, 
\begin{align}
\lim_{n\to \infty} l_n(\theta)/n &= \int_{-\infty}^{\infty} p(x|\theta_0)\log \left(\frac{p(x| \theta)}{ p(x| \theta_0)}\right) \, dx
\end{align}
となる. 右辺はサンプルを生成した分布と, 統計モデルのカルバック・ライブラ情報量の$-1$倍となっている. そのため, サンプルを生成した分布と統計モデルのカルバック・ライブラ情報量を最小にするためには $l_n(\theta)$ を最大にすればよいことが予想される.
$l_n(\theta)$ の式を少し変形する. 
\begin{align}
 l_n(\theta)= \sum_{i=1}^{n}\log p(x| \theta) \, dx -  \sum_{i=1}^{n}\log p(x_i| \theta_0)\, dx.
 \label{eqln}
\end{align}
右辺第2項は, サンプルを生成した分布のみによって定まる量であり, 統計モデルのパラメータ $\theta$ の選び方に依存しない. 
よって, $l_n(\theta)$ を最大にするためには, 第1項を最大にする $\theta$ を探せばよい. これが最尤法と呼ばれる推定方法のアイデアである.

$l_n（\theta）$ を最大化する $\theta$ は, 確率変数としてどのような振る舞いをするだろうか. 
そのことを調べるために, テイラー展開を使う. 
関数 $f(x)$ の $x_0$のまわりでのテイラー展開は, 
\begin{align}
f(x)= f(x_0) +f'(x_0) (x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 + \cdots 
\end{align}
であった. テイラー展開についてよく知らない場合は（なにかいい本）を参考にするとよい. 
$l_n（\theta）$ の $\theta_0$ の周りでのテイラー展開は, 
\begin{align}
l_n(\theta)= l_n(\theta_0) +l_n'(\theta_0) (\theta-\theta_0) + \frac{1}{2}l_n''(\theta_0)(\theta-\theta_0)^2 + \cdots 
\end{align}
である. 
$h/\sqrt{n}=\theta-\theta_0$と置くと, 
\begin{align}
l_n(\theta_0 + h/\sqrt{n})= l_n(\theta_0) +\frac{l_n'(\theta_0)}{\sqrt{n}}h+ \frac{1}{2n}l_n''(\theta_0)h^2 + \cdots 
\end{align}
となる. $h$ を $\sqrt{n}$ で割ったのは後の計算の便宜のためである.
$l_n(\theta_0) = \log{1} =0$ であるから, 
\begin{align}
l_n(\theta _0+ h/\sqrt{n})= \frac{l_n'(\theta_0)}{\sqrt{n}}h+ \frac{1}{2n}l_n''(\theta_0)h^2 + O(1/\sqrt{n})
\end{align}
と書ける.  ここで $O$ はランダウの記号である. ランダウの記号については（なにかいい本）を参考にするとよい. 
$l_n(\theta)$ の定義に戻ると, 
\begin{align}
&l_n(\theta_0 + h/\sqrt{n})  \nonumber \\
&\approx \frac{1}{\sqrt{n}} \left[\sum_{i=1}^{n} \left( \frac{d}{d \theta}  \log p(x_i | \theta) |_{\theta=\theta_0} \right)\right] h+ \frac{1}{2n}\left[  \sum_{i=1}^{n}\left(\frac{d^2}{d \theta^2} \log p(x_i | \theta)|_{\theta=\theta_0}\right)\right] h^2. 
\end{align}
スコア関数とフィッシャー情報行列についての性質を思い出すと, 大数の法則と中心極限定理より $n$ が十分大きいとき,  標準正規分布に従う確率変数 $Z$ を用いて, 
\begin{align}
l_n(\theta_0 + h/\sqrt{n}) &\approx Z \sqrt{I(\theta_0)} h  - \frac{I(\theta_0) h^2}{2} \\
&= -\frac{I(\theta_0)}{2}\left(h - \frac{Z}{\sqrt{I(\theta_0)}}\right)^2 + \frac{Z^2}{2}
\end{align}
という近似が成り立つ. $\theta=\theta_0 + h/\sqrt{n}$であったので,
\begin{align}
l_n(\theta)&= -\frac{I(\theta_0)}{2}\left(\sqrt{n}(\theta - \theta_0 )- \frac{Z}{\sqrt{I(\theta_0)}}\right)^2 + \frac{Z^2}{2}
\label{reslik}
\end{align}
右辺は2次関数であり, $\sqrt{n}(\theta - \theta_0) = Z/\sqrt{I(\theta_0)}$ のとき最大になる.
よって $l_n(\theta)$ を最大にするよう $\theta$ を決めると, $\sqrt{n}(\theta - \theta_0)$ は平均 0, 分散 $I(\theta_0)^{-1}$ の正規分布に従う.
これが最尤法の基礎である.
最尤法により推定された $\theta$ を最尤推定量と呼ぶ. 最尤推定量は確率変数であるから, サンプルを生成した分布のパラメータそのものではない. 
そこで最尤推定量は $\hat \theta$ などの記号を用いて, パラメータと区別する. 

より直感的に述べると, 最尤推定量 $\hat \theta$ はサンプルサイズが十分大きいとき, 平均 $\theta_0$, 分散 $I(\theta_0)^{-1}/\sqrt{n}$ の正規分布に従うということである. 

\paragraph{最尤推定の手順.} 
\begin{enumerate}
\item データ$X$を生成した分布に対し, 同時確率（密度）関数$p(X|\theta)$を適当に与える. 
\item $p(X|\theta)$ を $\theta$ の関数とみて, $p(X|\theta)$ を最大化する $\theta$ をなんらかの方法で探し, 推定量とする.
\end{enumerate}
$p(X|\theta)$ を $\theta$ の関数とみるときは尤度関数と呼ばれる. その対数をとったもの $\log p(X|\theta)$ を対数尤度関数と呼ぶ. 対数を取ることは多くの場合, 計算をとても便利にする. 

\subsubsection{例題}
まずは二項分布, ポアソン分布, 指数分布などで練習すると良い.

\subsection{パラメータが複数の場合}

対象とする確率分布がパラメータを複数持つ場合, 多変数のテイラー展開を用いることで, パラメータが1つの場合と同様の議論を展開することができる. 



\subsection{間違ったモデルで最尤推定すること}
上記ではサンプルを生成した分布と統計モデルが, パラメータのとり方によっては厳密に一致する場合を論じた. しかし, 現実にはサンプルを生成した分布は未知であり, 統計モデルは分析者が設定する. そのため, 統計モデルによって, サンプルを生成した分布が実現可能かどうかはわからない. 

最尤推定は真の分布とモデルの間のカルバック・ライブラ情報量を経験的に最小化する方法であったことを思い出すと, 最尤法は「間違ったモデル」を選んでも, 選んだ範囲内で（カルバック・ライブラ情報量の意味で）適切な推定を行うことが予想される. 

しかし, このことを一般的に保証するような定理は今のところないと思われる. 

\subsubsection{例題}
サンプルを生成した分布が確率密度関数
\begin{align}
g(x) = \frac{1}{6}x^3 exp(-x)
\end{align}
を持つとする. （この分布はガンマ分布と呼ばれる.）
統計モデルを正規分布とする. 

サンプルを生成した分布と統計モデルの間のカルバック・ライブラ情報量は, 
 \begin{align}
 E_g (-\log6+3\log X-X)-\frac{1}{2}E_g\{\log(2\pi \sigma^2)) + \frac{(X-\mu)^2}{\sigma^2}\}
 \end{align}
 である. ここで, $E_g[X] = \int xg(x)\, dx$.
 
（途中計算は省略して）

$\hat \mu=4$,　$\hat \sigma^2=4$で, 真の分布とモデルの間のカルバック・ライブラ距離は最小となる. 
R を使って最尤推定のシミュレーションをしてみる.
\begin{verbatim}
MLEnorm_sim <- function(i,n){
  x <- rgamma(n,4,1)
  muhat <- mean(x)
  n <- length(x)
  s2hat <-  n*var(x)/(n-1)
  c(muhat,s2hat)
}

library(parallel)
res10 <- t(simplify2array(mclapply(1:10000,MLEnorm_sim,n=10,
 mc.cores = detectCores())))
res100 <- t(simplify2array(mclapply(1:10000,MLEnorm_sim,n=100,
 mc.cores = detectCores())))
res1000 <- t(simplify2array(mclapply(1:10000,MLEnorm_sim,n=1000,
 mc.cores = detectCores())))

boxplot(cbind("n=10"=res10[,1],"n=100"=res100[,1],
 "n=1000"=res1000[,1]),main=expression(hat(mu)))
abline(h=4,lty=2)

boxplot(cbind("n=10"=res10[,2],"n=100"=res100[,2],
 "n=1000"=res1000[,2]),main=expression(hat(sigma^2)))
abline(h=4,lty=2)
\end{verbatim}

\section{カイ二乗分布を使った検定}

$Z$ を標準正規分布に従う確率変数とすると $Z^2$ は自由度1のカイ二乗分布に従う. 
自由度 $r$ のカイ二乗分布とは, 以下の確率密度関数を持つ分布である. 
\begin{align}
f(x) = \frac{1}{2^{r/2}\Gamma(r/2)}x^{r/2-1}\exp(-x/2).
\end{align}

また $Z_1, Z_2, \ldots, Z_r$ が独立でそれぞれ標準正規分布に従うとき, $Z_1^2+ Z_2^2+ \cdots +Z_r^2$ は自由度 $r$ のカイ二乗分布に従う. 

(\ref{reslik}) 式の結果を思い出すと, 対数尤度比関数 $l_n(\theta)$ が最大になるとき, その最大値の2倍は $Z^2$ であった. このことから, 尤度関数を用いた検定にカイ二乗分布が使えそうなことがわかる. 

\subsection{Wilks の定理}
\label{secWilks}
あとで書く. 

\subsection{$2 \times 2$分割表の独立性の検定}
様々な分野で表\ref{2by2}のようなカウントデータの表を考えることがある. このとき, 各行（暴露の有無）と各列（疾病の有無）は独立か, という問いに関心があることが多い. 
ここでいう独立とは, ...

\begin{table}[htbp]
\centering
\caption{$2 \times 2$分割表.}
\label{2by2}
\begin{tabular}{l|cc|c}
 & 疾病あり&疾病なし&合計\\
 \hline
 暴露あり&$a_{11}$&$a_{12}$&$N_1$\\
 暴露なし&$a_{21}$&$a_{22}$&$N_2$\\
  \hline
 合計　　& $M_1$ & $M_2$ & $N$
\end{tabular}
\end{table}

このようにシンプルな例でも, 統計モデルは複数考えることができる. 以下では, この分割表を$A=(a_{ij})$と表記する. 

\subsection{二項分布モデル}
$a_{i1}$ がそれぞれ独立にパラメータ $q_i$ の二項分布に従うとする.  
$Q=(q_i)$ とおく. 
$A$ が生じる確率$p(A|Q)$は, 
\begin{align}
p(A|Q) = \prod_{i=1}^2 \frac{N_i!}{a_{i1}!a_{i2}!}( q_i^{a_{i1}}(1-q_{i})^{a_{i2}})
\end{align}
とかける.
この場合, $q_i$ の最尤推定量は, $\hat q_{i}=a_{i1}/N_i$ である. 

分割表の各行と各列は独立であるとするモデルを, パラメータが $q_0=q_1=q_2$ という条件を満たしているモデルだとする. 
この場合, $q_0$ の最尤推定量は, $\hat q_{0}=M_1/N$ である.

次にこの2つのモデルの対数尤度比統計量 $G$ を考える.
\begin{align}
G =& 2 \left(\sum_{i=1}^2 \left\{ a_{i1}\log(\hat q_i)+ a_{i2}\log(1-\hat q_i)\right\} -\right. \\
&\left.\sum_{i=1}^2 \left\{ a_{i1}\log(\hat q_0) + a_{i2}\log(1-\hat q_0)\right\}\right)\\
&= 2 \left(\sum_{i=1}^2\left\{ a_{i1}\log\frac{\hat q_i}{\hat q_0} + a_{i2}\log\frac{1-\hat q_i}{1-\hat q_0}\right\}\right)
\end{align}
ここで $\hat q_i/\hat q_0 = a_{i1}/(N_iM_i/N)$,  $(1-\hat q_i)/(1-\hat q_0) = a_{i1}/(N_iM_i/N)$ と書ける.
$a_{ij}$ を観測度数, $N_iM_i/N$ や $N_iM_i/N$ を理論度数と呼ぶことがある.
この統計量 $G$ を用いた検定を $G$ 検定と呼ぶことがある.



\ref{secWilks}節で述べた通り, $G$はサンプルサイズが十分大きいとき, 自由度1のカイ二乗分布に近似的に従う. 

\subsubsection{ポアソン分布モデル}
%$\Lambda = (\lambda_{ij})$ を正の実数を成分とする$2 \times 2$行列とする. 
%$a_{ij}$ がそれぞれ独立にパラメータ $\lambda_{ij}$ のポアソン分布に従うとすると, $A$ が生じる確率$p(A|\Lambda)$は, 
%\begin{align}
%p(A|\Lambda) = 
%\prod_{i,j} \frac{e^{-\lambda_{ij}} \lambda_{ij}^{a_{ij}}}{a_{ij}!}
%\end{align}
%と書ける. 
%分割表の各行と各列は独立であるとするモデルは, 行についてのパラメータ$\mu_i$と列についてのパラメータ$\nu_j$, 

\paragraph{注意点.}
統計的仮説検定は確率版背理法に例えられることもあるが, 現実のデータを扱う以上, 背理法のようにすっきりとはいかない. 具体的には, 以下のような難しさがある.

\section{回帰型の統計モデル}

\begin{thebibliography}{1}
\bibitem{Kuroki2017}黒木玄. 2017-06-04 LindebergのTaylor展開のみを使った中心極限定理の証明.pdf. \url{https://genkuroki.github.io/documents/mathtodon/}.
\bibitem{Akaike1980} 赤池弘次. (1980). エントロピーとモデルの尤度 (< 講座> 物理学周辺の確率統計). 日本物理学会誌, 35(7), 608-614.
\end{thebibliography}
%$L_n（\theta）$を対数尤度関数と呼び, $L_n（\theta）$ を最大にするよう, パラメータ $\theta$ を選ぶ方法を最尤法と呼ぶ.
%対数を取らずに, $ \prod_{i=1}^{n}p(x_i| \theta) \, dx $ を最大化する方法を最尤法として定義し, 対数を取るのは計算上のテクニックだとする解説も多い
\end{document}
