\documentclass[a4paper, 12pt]{jarticle}
\usepackage{amsmath, amsthm, url}
\title{私のための「頻度論」統計学入門}
\author{阿部 興\footnote{「あべ こう」と読む}}
\西暦
\begin{document}
\maketitle

\theoremstyle{definition}
\newtheorem{theorem}{定理}
\newtheorem*{theorem*}{定理}
\newtheorem{definition}[theorem]{定義}
\newtheorem*{definition*}{定義}

\newcommand{\KL}{D_{\mathrm KL}}

\section*{記法}

$\log(x)$ は自然対数を表す.

\section{確率の話}
ここでは主に大数の法則, 中心極限定理, カルバック・ライブラ情報量について勉強する.

\subsection{大数の法則}

いろいろな本に書いてある. 

\subsection{中心極限定理}

中心極限定理の証明には, 特性関数やモーメント母関数を使うことが多い. 

テイラー展開を使ったより簡単な証明が, 黒木 (2017)\cite{Kuroki2017} にある. 

\subsection{カルバック・ライブラ情報量}
統計的推測のために重要な役割を果たす量の1つにカルバック・ライブラ情報量がある.
カルバック・ライブラ情報量$\KL(p\|q)$は離散型の確率関数$p(x)$と$q(x)$に対しては, 
\begin{align}
\KL(p\|q) = -\sum_{x} p(x) \log \frac{q(x)}{p(x)}
\end{align}
連続型の確率密度関数$p(x)$と$q(x)$に対しては, 
\begin{align}
\KL(p\|q) = -\int p(x) \log \frac{q(x)}{p(x)} \, dx
\end{align}
と定義される. 
赤池（1980）\cite{Akaike1980} に習って, カルバック・ライブラ情報量の直感的な意味を考える. 結論を先取りすると, カルバック・ライブラ情報量は「サンプルを生成した分布が $q(x)$ のとき, 経験分布がほぼ $p(x)$ となる確率の対数のサンプルサイズ分の 1 の符号反転」と解釈できる. このカギカッコの中身の意味がわかるようになることがこの節の目標である.

状態 $i=1,\ldots,M$ がそれぞれ $q_i$ の確率で生起する分布と, 状態 $i=1,\ldots,M$ がそれぞれ $p_i$ の確率で生起する分布を考える. この分布からのサンプルを $N$ 個観測して、状態 $i=1,\ldots,M$ が生起した回数をそれぞれ $N_1,\ldots,N_M$ とする. $ N=\sum_{i=1}^{M}N_i$ である. 
$N_1,\ldots,N_M$ のような観測が得られる確率は,
\begin{align}
W=\frac{N!}{N_1! \cdots N_M!}q^{N_1}_1 \cdots q^{N_M}_M
\end{align}
と表せる. （このような分布は多項分布と呼ばれる.）

ここでスターリングの公式
\begin{align}
\log N! \approx N\log N -N
\end{align}
を使って $\log W$ を近似すると
\begin{align}
\log W &\approx (N \log N) - \sum_{i=1}^{M}( N_i \log N_i -N_i) + \sum_i=1^{M} \log q_i \\
&= N \log N - \sum_{i=1}^M N_i (\log N_i - q_i) \\
&= - \sum_{i=1}^M N_i (\log N_i - q_i -\log N ) \\
&= - \sum_{i=1}^M N_i (\log \frac{N_i}{N} - q_i) \\
&= - N \sum_{i=1}^M \frac{N_i}{N} (\log \frac{N_i}{N} - q_i) 
\end{align}
と整理できる.
$p_i = N_i/N$ とおくと
\begin{align}
\log W \approx -N\sum_{i=1}^{M}p_i \log\frac{p_i}{q_i}
\end{align}
という結果を得る.

あらためて考えると $p_i = N_i/N$  は経験的に推定された確率と解釈できる.
$q_i$ は真の確率であったことを思い出すと、$W$ はサンプルを生成した真の分布が $q_i$ のときに $p_i$ のように振る舞う確率と解釈できる. 
$\log W$を $N$ で割り, 符号を反転させると, $-\sum_{i=1}^{M} p_i \log {\frac {q_i}{p_i}}$ となる.
この量は「母集団分布が $q_i$ のとき経験分布がほぼ $p_i$ となる確率の対数のサンプルサイズ分の 1 の符号反転」と解釈できる.

カルバック・ライブラ情報量は次の性質を満たすため, 分布の近さを測る指標となる. 
\begin{align}
\KL(p\|q) \ge 0,
\end{align}
かつ, $\KL(p\|q) = 0$となるのは $p(x)=q(x)$のときに限られる.

\paragraph{注意.}
定義をよく見ればわかるように, カルバック・ライブラ情報量では $\KL(p\|q) =\KL(q\|p)$ は成り立たない. 

%\subsection{サノフの定理}

\section{最尤法}
本節では最尤法と呼ばれる方法の性質について述べる. これについて理解するために, フィッシャー情報量と呼ばれる量が重要になるため, 先にフィッシャー情報量についての性質を述べる. 

\subsection{パラメータが1つの場合}

\subsubsection{スコア関数とフィッシャー情報量}
パラメータ $\theta$ を持つ確率（密度）関数 $p(x|\theta_0)$ について, スコア関数 $S(\theta)$ を次のように定義する. 
\begin{align}
S(\theta) = \frac{d}{d\theta}\log p(x | \theta). 
\end{align}
スコア関数の $p(x|\theta)$ による平均は 0 である.  
\begin{align}
&\int_{-\infty}^{\infty} \frac{d}{d\theta}\log p(x | \theta) p(x|\theta)\, dx \\
&= \int_{-\infty}^{\infty} \frac{\frac{d}{d\theta}p(x | \theta)}{p(x | \theta)} p(x|\theta) \, dx \\
&=\int_{-\infty}^{\infty} \frac{d}{d\theta}p(x | \theta)\, dx \\
&=  \frac{d}{d\theta} \int_{-\infty}^{\infty} p(x | \theta)\, dx\\
&= 0.
\end{align}
従い, スコア関数の分散はスコア関数の2乗の平均に等しい.

フィッシャー情報量を
\begin{align}
I(\theta)=-\int^{\infty}_{-\infty}\left(\frac{d^2}{d \theta^2} \log p(x_i | \theta)\right) p(x|\theta) \, dx
\end{align}
と定義すると, 
\begin{align}
I(\theta)&=-\int^{\infty}_{-\infty}\frac{d}{d \theta} \left(\left(\frac{\frac{d}{d\theta} p(x_i | \theta)}{p(x|\theta)}\right) p(x|\theta) \right) \, dx \\
&=-\int^{\infty}_{-\infty}\left(\left(\frac{\frac{d^2}{d\theta^2} p(x_i | \theta)}{p(x|\theta)} - \frac{ ( \frac{d}{d\theta} p(x_i | \theta))^2}{p(x|\theta)^2 }\right) p(x|\theta) \right) \, dx \\
&=-\int^{\infty}_{-\infty}\left(\frac{\frac{d^2}{d\theta^2} p(x_i | \theta)}{p(x|\theta)}\right) p(x|\theta) \, dx +\int^{\infty}_{-\infty}\left( \frac{ ( \frac{d}{d\theta} p(x_i | \theta))^2}{p(x|\theta)^2 }\right) p(x|\theta) \, dx
\end{align}
スコア関数のときと同様, 第1項は消える. 
第2項は
\begin{align}
\int^{\infty}_{-\infty}\left(\frac{d}{d\theta} \log p(x | \theta)\right)^2 p(x|\theta) \, dx 
\end{align}
と等しい. これはスコア関数の2乗の平均になっている. 
すなわち, スコア関数の分散はフィッシャー情報行列と等しいことがわかった. 

\subsubsection{最尤推定量の性質}
\label{secPropertyMLE}
サンプル $x_i$ ($i=1,\ldots n$) が, 独立に同一の確率（密度）関数 $p(x|\theta_0)$ を持つ分布から得られたとする\footnote{ 「独立に同一の確率分布に従う」ことを　i.i.d. (independent and identically distributed) と略すことがある.}. ここで $\theta$ は確率（密度）関数のパラメータである\footnote{このパラメータのことを母数と呼ぶことがあるが, 母数という語は誤解を招くことが多いので, 本稿ではあまりつかわない.}.  
このようなデータに対し, 統計モデル $p(x|\theta)$ を考え, 未知パラメータの $\theta$ を推定したい.

まず, 次の対数尤度比関数 $l_n(\theta)$ を考える. 
\begin{align}
l_n(\theta) = \log \left( \frac{\prod_{i=1}^{n}p(x_i|\theta)}{\prod_{i=1}^{n}p(x_i|\theta_0)} \right) = \sum_{i=1}^{n}\log \left(\frac{\log p(x_i| \theta)}{\log p(x_i| \theta_0)}\right).
\end{align}
これをサンプルサイズ（標本の大きさ） $n$ で割ると大数の法則により, 
\begin{align}
\lim_{n\to \infty} l_n(\theta)/n &= \int_{-\infty}^{\infty} p(x|\theta_0)\log \left(\frac{p(x| \theta)}{ p(x| \theta_0)}\right) \, dx
\end{align}
となる. 右辺はサンプルを生成した分布と, 統計モデルのカルバック・ライブラ情報量の$-1$倍となっている. そのため, サンプルを生成した分布と統計モデルのカルバック・ライブラ情報量を最小にするためには $l_n(\theta)$ を最大にすればよいことが予想される.
$l_n(\theta)$ の式を少し変形する. 
\begin{align}
 l_n(\theta)= \sum_{i=1}^{n}\log p(x| \theta) \, dx -  \sum_{i=1}^{n}\log p(x_i| \theta_0)\, dx.
 \label{eqln}
\end{align}
右辺第2項は, サンプルを生成した分布のみによって定まる量であり, 統計モデルのパラメータ $\theta$ の選び方に依存しない. 
よって, $l_n(\theta)$ を最大にするためには, 第1項を最大にする $\theta$ を探せばよい. これが最尤法と呼ばれる推定方法のアイデアである.

$l_n（\theta）$ を最大化する $\theta$ は, 確率変数としてどのような振る舞いをするだろうか. 
そのことを調べるために, テイラー展開を使う. 
関数 $f(x)$ の $x_0$のまわりでのテイラー展開は, 
\begin{align}
f(x)= f(x_0) +f'(x_0) (x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 + \cdots 
\end{align}
であった. テイラー展開についてよく知らない場合は（なにかいい本）を参考にするとよい. 
$l_n（\theta）$ の $\theta_0$ の周りでのテイラー展開は, 
\begin{align}
l_n(\theta)= l_n(\theta_0) +l_n'(\theta_0) (\theta-\theta_0) + \frac{1}{2}l_n''(\theta_0)(\theta-\theta_0)^2 + \cdots 
\end{align}
である. 
$h/\sqrt{n}=\theta-\theta_0$と置くと, 
\begin{align}
l_n(\theta_0 + h/\sqrt{n})= l_n(\theta_0) +\frac{l_n'(\theta_0)}{\sqrt{n}}h+ \frac{1}{2n}l_n''(\theta_0)h^2 + \cdots 
\end{align}
となる. $h$ を $\sqrt{n}$ で割ったのは後の計算の便宜のためである.
$l_n(\theta_0) = \log{1} =0$ であるから, 
\begin{align}
l_n(\theta _0+ h/\sqrt{n})= \frac{l_n'(\theta_0)}{\sqrt{n}}h+ \frac{1}{2n}l_n''(\theta_0)h^2 + O(1/\sqrt{n})
\end{align}
と書ける.  ここで $O$ はランダウの記号である. ランダウの記号については（なにかいい本）を参考にするとよい. 
$l_n(\theta)$ の定義に戻ると, 
\begin{align}
&l_n(\theta_0 + h/\sqrt{n})  \nonumber \\
&\approx \frac{1}{\sqrt{n}} \left[\sum_{i=1}^{n} \left( \frac{d}{d \theta}  \log p(x_i | \theta) |_{\theta=\theta_0} \right)\right] h+ \frac{1}{2n}\left[  \sum_{i=1}^{n}\left(\frac{d^2}{d \theta^2} \log p(x_i | \theta)|_{\theta=\theta_0}\right)\right] h^2. 
\end{align}
スコア関数とフィッシャー情報行列についての性質を思い出すと, 大数の法則と中心極限定理より $n$ が十分大きいとき,  標準正規分布に従う確率変数 $Z$ を用いて, 
\begin{align}
l_n(\theta_0 + h/\sqrt{n}) &\approx Z \sqrt{I(\theta_0)} h  - \frac{I(\theta_0) h^2}{2} \\
&= -\frac{I(\theta_0)}{2}\left(h - \frac{Z}{\sqrt{I(\theta_0)}}\right)^2 + \frac{Z^2}{2}
\end{align}
という近似が成り立つ. $\theta=\theta_0 + h/\sqrt{n}$であったので,
\begin{align}
l_n(\theta)&= -\frac{I(\theta_0)}{2}\left(\sqrt{n}(\theta - \theta_0 )- \frac{Z}{\sqrt{I(\theta_0)}}\right)^2 + \frac{Z^2}{2}
\label{reslik}
\end{align}
右辺は2次関数であり, $\sqrt{n}(\theta - \theta_0) = Z/\sqrt{I(\theta_0)}$ のとき最大になる.
よって $l_n(\theta)$ を最大にするよう $\theta$ を決めると, $\sqrt{n}(\theta - \theta_0)$ は平均 0, 分散 $I(\theta_0)^{-1}$ の正規分布に従う.
これが最尤法の基礎である.
最尤法により推定された $\theta$ を最尤推定量と呼ぶ. 最尤推定量は確率変数であるから, サンプルを生成した分布のパラメータそのものではない. 
そこで最尤推定量は $\hat \theta$ などの記号を用いて, パラメータと区別する. 

より直感的に述べると, 最尤推定量 $\hat \theta$ はサンプルサイズが十分大きいとき, 平均 $\theta_0$, 分散 $I(\theta_0)^{-1}/n$ の正規分布に従うということである. 

\paragraph{最尤推定の手順.} 
\begin{enumerate}
\item データ$X$を生成した分布に対し, 同時確率（密度）関数$p(X|\theta)$を適当に与える. 
\item $p(X|\theta)$ を $\theta$ の関数とみて, $p(X|\theta)$ を最大化する $\theta$ をなんらかの方法で探し, 推定量とする.
\end{enumerate}
$p(X|\theta)$ を $\theta$ の関数とみるときは尤度関数と呼ばれる. その対数をとったもの $\log p(X|\theta)$ を対数尤度関数と呼ぶ. 対数を取ることは多くの場合, 計算をとても便利にする. 

\subsubsection{例題}
まずは二項分布, ポアソン分布, 指数分布などで練習すると良い.

\subsection{パラメータが複数の場合}

対象とする確率分布がパラメータを複数持つ場合, 多変数のテイラー展開を用いることで, パラメータが1つの場合と同様の議論を展開することができる. 



\subsection{間違ったモデルで最尤推定すること}
上記ではサンプルを生成した分布と統計モデルが, パラメータのとり方によっては厳密に一致する場合を論じた. しかし, 現実にはサンプルを生成した分布は未知であり, 統計モデルは分析者が設定する. そのため, 統計モデルによって, サンプルを生成した分布が実現可能かどうかはわからない. 

最尤推定は真の分布とモデルの間のカルバック・ライブラ情報量を経験的に最小化する方法であったことを思い出すと, 最尤法は「間違ったモデル」を選んでも, 選んだ範囲内で（カルバック・ライブラ情報量の意味で）適切な推定を行うことが予想される. 

しかし, このことを一般的に保証するような定理は今のところないと思う. 

\subsubsection{例題}
サンプルを生成した分布が確率密度関数
\begin{align}
g(x) = \frac{1}{6}x^3 exp(-x)
\end{align}
を持つとする. （この分布はガンマ分布と呼ばれる分布の一例である.）
統計モデルを正規分布とする. 

サンプルを生成した分布と統計モデルの間のカルバック・ライブラ情報量は, 
 \begin{align}
 E_g (-\log6+3\log X-X)-\frac{1}{2}E_g\{\log(2\pi \sigma^2)) + \frac{(X-\mu)^2}{\sigma^2}\}
 \end{align}
 である. ここで, $E_g[X] = \int xg(x)\, dx$.
 
（途中計算は省略して）

$\hat \mu=4$,　$\hat \sigma^2=4$で, 真の分布とモデルの間のカルバック・ライブラ距離は最小となる. 
R を使って最尤推定のシミュレーションをしてみる.
\begin{verbatim}
MLEnorm_sim <- function(i,n){
  x <- rgamma(n,4,1)
  muhat <- mean(x)
  n <- length(x)
  s2hat <-  n*var(x)/(n-1)
  c(muhat,s2hat)
}

library(parallel)
res10 <- t(simplify2array(mclapply(1:10000,MLEnorm_sim,n=10,
 mc.cores = detectCores())))
res100 <- t(simplify2array(mclapply(1:10000,MLEnorm_sim,n=100,
 mc.cores = detectCores())))
res1000 <- t(simplify2array(mclapply(1:10000,MLEnorm_sim,n=1000,
 mc.cores = detectCores())))

boxplot(cbind("n=10"=res10[,1],"n=100"=res100[,1],
 "n=1000"=res1000[,1]),main=expression(hat(mu)))
abline(h=4,lty=2)

boxplot(cbind("n=10"=res10[,2],"n=100"=res100[,2],
 "n=1000"=res1000[,2]),main=expression(hat(sigma^2)))
abline(h=4,lty=2)
\end{verbatim}

\section{カイ二乗分布を使った検定}

$Z$ を標準正規分布に従う確率変数とすると $Z^2$ は自由度1のカイ二乗分布に従う. 
自由度 $r$ のカイ二乗分布とは, 以下の確率密度関数を持つ分布である. 
\begin{align}
f(x) = \frac{1}{2^{r/2}\Gamma(r/2)}x^{r/2-1}\exp(-x/2).
\end{align}

また $Z_1, Z_2, \ldots, Z_r$ が独立でそれぞれ標準正規分布に従うとき, $Z_1^2+ Z_2^2+ \cdots +Z_r^2$ は自由度 $r$ のカイ二乗分布に従う. 

(\ref{reslik}) 式の結果を思い出すと, 対数尤度比関数 $l_n(\theta)$ が最大になるとき, その最大値の2倍は $Z^2$ であった. このことから, 尤度関数を用いた検定にカイ二乗分布が使えそうなことがわかる. 

\subsection{Wilks の定理}
\label{secWilks}
あとで書く. 

\subsection{$2 \times 2$分割表の独立性の検定}
様々な分野で表\ref{2by2}のようなカウントデータの表を考えることがある. このとき, 各行（暴露の有無）と各列（疾病の有無）は独立か, という問いに関心があることが多い. 
ここでいう独立とは, ...

\begin{table}[htbp]
\centering
\caption{$2 \times 2$分割表.}
\label{2by2}
\begin{tabular}{l|cc|c}
 & 疾病あり&疾病なし&合計\\
 \hline
 暴露あり&$a_{11}$&$a_{12}$&$N_1$\\
 暴露なし&$a_{21}$&$a_{22}$&$N_2$\\
  \hline
 合計 & $M_1$ & $M_2$ & $N$
\end{tabular}
\end{table}

このようにシンプルな例でも, 統計モデルは複数考えることができる. 以下では, この分割表を$A=(a_{ij})$と表記する. 

\subsection{二項分布モデル}
$a_{i1}$ がそれぞれ独立にパラメータ $q_i$ の二項分布に従うとする.  
$Q=(q_i)$ とおく. 
$A$ が生じる確率$p(A|Q)$は, 
\begin{align}
p(A|Q) = \prod_{i=1}^2 \frac{N_i!}{a_{i1}!a_{i2}!}( q_i^{a_{i1}}(1-q_{i})^{a_{i2}})
\end{align}
とかける.
この場合, $q_i$ の最尤推定量は, $\hat q_{i}=a_{i1}/N_i$ である. 

分割表の各行と各列は独立であるとするモデルを, パラメータが $q_0=q_1=q_2$ という条件を満たしているモデルだとする. 
この場合, $q_0$ の最尤推定量は, $\hat q_{0}=M_1/N$ である.

次にこの2つのモデルの対数尤度比統計量 $G_0$ を考える.
\ref{secWilks}節で述べた通り, $G_0$はサンプルサイズが十分大きいとき, 自由度1のカイ二乗分布に近似的に従う. 
\begin{align}
G_0 =& 2 \left(\sum_{i=1}^2 \left\{ a_{i1}\log(\hat q_i)+ a_{i2}\log(1-\hat q_i)\right\} -\right. \\
&\left.\sum_{i=1}^2 \left\{ a_{i1}\log(\hat q_0) + a_{i2}\log(1-\hat q_0)\right\}\right)\\
&= 2 \left(\sum_{i=1}^2\left\{ a_{i1}\log\frac{\hat q_i}{\hat q_0} + a_{i2}\log\frac{1-\hat q_i}{1-\hat q_0}\right\}\right)
\end{align}
ここで $\hat q_i/\hat q_0 = a_{i1}/(N_iM_i/N)$,  $(1-\hat q_i)/(1-\hat q_0) = a_{i1}/(N_iM_i/N)$ と書ける.
この統計量 $G_0$ を用いた検定を $G$ 検定と呼ぶことがある.
また$a_{ij}$ を観測度数, $N_iM_i/N$ や $N_iM_i/N$ を理論度数と呼ぶことがある.
$2 \times 2$分割表の独立性の検定において, 理論度数を表\ref{2by2theo} のように定めると, 統計量 $G$ は次のようにも書ける.
\begin{align}
G = 2 \left(\sum_{i,j} a_{ij}\log\frac{a_{ij}}{E_ij} \right).
\end{align}
\begin{table}[htbp]
\centering
\caption{$2 \times 2$分割表の独立性の検定における理論度数.}
\label{2by2theo}
\begin{tabular}{l|cc|c}
 & 疾病あり&疾病なし&合計\\
 \hline
 暴露あり&$E_{11} = N_1M_1/N$&$E_{12} = N_1M_2$&$N_1$\\
 暴露なし&$E_{21} = M_1N_2/N $&$E_{22}=M_2N_2/N$&$N_2$\\
  \hline
 合計 & $M_1$ & $M_2$ & $N$
\end{tabular}
\end{table}

ところで, $\log (1+x)$ を0のまわりでテイラー展開することで次の式が成り立つ. 
\begin{align}
(1+x) \log (1+x) &= (1+x)\left(x-\frac{x^2}{2} + O(x^3)\right) \\
& = x + \frac{x^2}{2} +  O(x^3)
\end{align}
2つめの等号において, かっこを展開すると $x^3$ についての項が出てくるが, $O(x^3)$ に吸収される.

この式を用いると, 統計量 $G$ の式は, 以下のように近似できる.
\begin{align}
&2 \left(\sum_{i,j} a_{ij}\log\frac{a_{ij}}{E_ij} \right)\\
&=2 \left(\sum_{i,j} E_{ij} \left(1+ \frac{a_{ij}-E_{ij}}{E_{ij}}\right)\log\left(1+ \frac{a_{ij}-E_{ij}}{E_ij} \right)\right)\\
&\approx 2 \left(\sum_{i,j} E_{ij}\left(  \frac{a_{ij}-E_{ij}}{E_{ij}}- \frac{(a_{ij}-E_{ij})^2}{E_{ij}^2} \right)\right) \\
&= 2 \left(\sum_{i,j} \frac{(a_{ij}-E_{ij})^2}{E_{ij}} \right).
\end{align}
最後の等号においては, $\sum_{i,j} E_{ij} =\sum_{i,j} a_{ij} = N$ を用いた.
この統計量を$\chi^2_0$とおく. 
$\chi^2_0$は$G_0$とサンプルサイズが十分大きいとき等しい.
すなわち, $\chi^2_0$ はサンプルサイズが自由度1のカイ二乗分布に近似的に従う.

単にカイ二乗検定というときは, この $\chi^2_0$ 統計量を用いた検定を指すことが多い. 

ではカイ二乗検定の手順を具体的に示そう. 

\paragraph{カイ二乗検定の手順.}
データから求めた検定統計量（この場合$G_0$や$\chi^2_0$）以上の値が, 自由度1のカイ二乗分布から出る確率を調べる. その調べた確率をp値と呼ぶ.
p値が小さすぎるとき, 分割表の各行と各列は独立でないと判断する.

どの程度の確率であれば, 小さすぎると判断するかは事前に定めておく. この事前に定めた閾を有意水準と呼ぶ.

\subsubsection{ポアソン分布モデル}
%$\Lambda = (\lambda_{ij})$ を正の実数を成分とする$2 \times 2$行列とする. 
%$a_{ij}$ がそれぞれ独立にパラメータ $\lambda_{ij}$ のポアソン分布に従うとすると, $A$ が生じる確率$p(A|\Lambda)$は, 
%\begin{align}
%p(A|\Lambda) = 
%\prod_{i,j} \frac{e^{-\lambda_{ij}} \lambda_{ij}^{a_{ij}}}{a_{ij}!}
%\end{align}
%と書ける. 
%分割表の各行と各列は独立であるとするモデルは, 行についてのパラメータ$\mu_i$と列についてのパラメータ$\nu_j$, 

\paragraph{注意点.}
統計的仮説検定は確率版背理法に例えられることもあるが, 現実のデータを扱う以上, 背理法のようにすっきりとはいかない. 
具体的にどのような難しさがあるかは 節で改めて詳しく述べる. 

\subsubsection{カイ二乗検定のシミュレーション}
$\chi^2_0$は$G_0$とサンプルサイズが十分大きいとき等しいと述べたが, サンプルサイズが小さいときには大きく性質がことなる.
どの程度のサンプルサイズのとき, どのような性質を持つかを調べるには, コンピューターを使ってシミュレーションするのが早い. 

よい仮説検定の第1条件は, まずなによりも$\alpha$エラーをコントロールできること, 第2の条件は検出力が高いことである. 

$G$ 検定はサンプルサイズの小さいとき, 実際の$\alpha$エラーが名目上の有意水準を上回ることが多い. 
カイ二乗検定は概ね正確な $p$ 値を与えることが多い.

\paragraph{注意点. } 本説の議論は, 一般の $r \times c$ 分割表にも拡張できる. しかし, $r \times c$ 分割表をカイ二乗検定で分析するのは, 私があまり好きではないので, 本稿では扱わない.
$r$ と $c$ が 2より大きいとき, $r \times c$ 分割表の独立性のカイ二乗検定では, 行と列が独立でないことは言えても, どの行とどの列に特に大きな関係があるかはわからない. 
そこでカイ二乗検定を行った後に, 残差分析と呼ばれる分析を行うことがある. しかし, このように検定を何段階も行うことの, 多重比較のような問題を考え出すと話がややこしくなるように思う.  
連続量を大, 中, 小のように分けて表にした場合は, 元の連続量に対する統計モデルを考えたほうがいいように思う. 
また, 順序尺度の量を集計した場合は,　順序尺度に対する統計モデルを考えたほうがいいように思う. 
順序尺度に対する統計モデルは, \ref{secCata} 節で扱う予定である. 

\section{ワルド検定とワルド信頼区間}


まず, 次の定理を紹介する. 

\begin{theorem}
$\hat \theta$ が最尤推定量であるとき, 関数 $g(\theta)$ の最尤推定量は $g(\hat \theta)$ である. これを最尤推定量の不変性という.
\end{theorem}
ただし, $g(\theta)$ が1対1の変換になっていないとき, この定理は成り立たない. 

自明な事実を述べたにすぎないと思われると思うので, この定理はなにがうれしいか補足する.

例えば正の値しか取らないパラメータ $\sigma$ を最優推定したいとき, 制約付きの最適化をするのは面倒である. このようなとき, $\sigma=\exp(\rho)$ とおいて, $\rho$ を最尤推定し, $\hat \sigma =\exp(\hat \rho)$ として構わない. 

\subsection{ワルド検定}
\ref{secPropertyMLE} 節で述べた通り, サンプルサイズが十分大きいとき, 最尤推定量は平均 $\theta$, 分散 $I(\theta)^{-1}/n$ の正規分布で近似できた. ここで $I(\theta)$ はフィッシャー情報量である. 
しかし, 仮に統計モデルとしてデータを生成した分布を実現可能な分布を選んだとしても\footnote{一般には, データを生成した分布を実現可能な分布ということはとても難しい. その方法はわかっていない. }, 正しいパラメータ $\theta$ は不明であり, 解析的な期待値の計算も難しいことが多い. 
 
そこでサンプル $x_i$ ($i=1,\ldots,n$) が得られたときの, 統計モデル $p(x|\theta)$ の観測情報量 $I_o(\theta)$ を次のように定義する.
\begin{align}
I_o(\theta)=\frac{1}{n}\sum_{i=1}^{n} \frac{\partial^2}{\partial \theta^2}\log p(x_i|\theta).
\end{align}
サンプルサイズ$n$が十分大きいとき, $I_o(\theta)$ は $I(\theta)$ に近づく. 
これにより, 最尤推定量は分散 $I_o(\hat \theta)^{-1}/n$ の正規分布で近似できる. 

\subsection{ワルド信頼区間}
ワルド信頼区間を次のように定義する.


\section{回帰型の統計モデル}
\subsection{ロジスティック回帰}
\subsection{ポアソン回帰}

\section{モデル選択}
\subsection{クロスバリデーション}

\paragraph{注意点.}
ディープラーニングのパッケージなどでは, バリデーション損失を簡単にモニタできるようになっていることが多い. これは便利であるが, 弊害もあると感じている.
例えばバリデーション損失をモニタしながら最適化を行い, そのバリデーション損失をそのまま予測精度として報告した場合, 間接的にバリデーション用のデータにフィットするように最適化を行っているのと同じことが起こる気がしている. 
チューニングパラメータの多いモデルの場合, 理想的には, チューニングパラメータを決めるためのバリデーションデータでチューニングを行い, 最終的な予測精度の評価はまた別のバリデーションデータで行うほうが良いと思う. しかしこの方法ではデータが大量に必要になる. 


\section{いくつかの統計モデルのカタログ}
\label{secCata}
統計モデルを紹介した楽しい本に松浦 や　須山　がある.
ここではこれらの本に記載されていない統計モデルを紹介する. これにより自分の手で統計モデルを作るということがどのようなことか, その雰囲気の一端を感じてもらいたい. 

\begin{thebibliography}{1}
\bibitem{Kuroki2017}黒木玄. 2017-06-04 LindebergのTaylor展開のみを使った中心極限定理の証明.pdf. \url{https://genkuroki.github.io/documents/mathtodon/}.
\bibitem{Akaike1980} 赤池弘次. (1980). エントロピーとモデルの尤度 (〈講座〉物理学周辺の確率統計). 日本物理学会誌, 35(7), 608-614.
\end{thebibliography}
%$L_n（\theta）$を対数尤度関数と呼び, $L_n（\theta）$ を最大にするよう, パラメータ $\theta$ を選ぶ方法を最尤法と呼ぶ.
%対数を取らずに, $ \prod_{i=1}^{n}p(x_i| \theta) \, dx $ を最大化する方法を最尤法として定義し, 対数を取るのは計算上のテクニックだとする解説も多い
\end{document}
